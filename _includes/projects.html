<table class="projects">
  <tbody>
    <!-- ! Multimodal Perception for Autonomous Racing -->
    <!-- <tr><td>
      <div class="project_cell">
        <img src="assets/img/multimodal_perception.png" width="250" class="zoom">
      </div>
      <div class="project_cell">
        <p style="font-size:18px;margin-bottom: 0px;">
          <font color="black"><b>Multimodal Perception for Autonomous Racing</b></font>
        </p>
        <p>
          [<a href="https://github.com/curious-ai/pytorch-td-rex/tree/offline" target="_blank">
            Code
          </a>]
          <br>
          Developed a lightweight UNet model for image segmentation with a Dice metric of 0.99 and streamlining data annotation using SAM. Enhanced sensor fusion by establishing ROS communication at 30 Hz for lidar and RGB cameras, and integrated the Dreamer algorithm with an Offline RL framework to achieve efficient online fine-tuning of the world model directly in the real world with a step ratio of less than 40%.
        </p>
      </div>
    </td></tr> -->
    <tr>
      <td>
        <div class="project_cell">
          <img src="assets/img/multimodal_perception_cover_gif.gif" width="250" height="200" class="zoom">
        </div>
        <div class="project_cell">
          <p style="font-size:18px;margin-bottom: 0px;">
            <font color="black"><b>Multimodal Perception for Autonomous Racing</b></font>
          </p>
          <p>
            [<span style="color: grey; text-decoration: none;">Code (Coming Soon)</span>]
            [<a href="https://drive.google.com/file/d/1stXqYokZfWw-clVRVku7g8p80hlcKNYL/view?usp=sharing" target="_blank">Video</a>]
            <br>
            To develop a robust multi-terrain control policy for autonomous racing, LiDAR and RGB camera inputs were used for online RL. A lightweight UNet model improved visual sim-to-real transfer, achieving a 0.99 Dice score by segmenting floor, opponent rover, walls, and background. ROS nodelet manager and optimized ROS-Docker communication boosted update rates from 10 Hz to 30 Hz, allowing the rover to run up to 2.5 m/s. Offline TD-MPC RL then yielded effective results with 5K-10K transitions in under 15 episodes, compared to over 300K episodes needed for online training.
          </p>
        </div>
      </td>
    </tr>
    <tr>
      <td>
        <div class="project_cell">
          <img src="assets/img/wombat_results_gif.gif" width="250" height="200" class="zoom">
        </div>
        <div class="project_cell">
          <p style="font-size:18px;margin-bottom: 0px;">
            <font color="black"><b>Electronic-waste Recycling</b></font>
          </p>
          <p>
            [<a href="https://github.com/biorobotics/Wombat_robosuite" target="_blank">Code</a>]
            [<a href="https://docs.google.com/presentation/d/1uCb-5oLUr9LShpgxhDzhjrvFSEvU9o_i/edit?usp=sharing&ouid=105419178815487827350&rtpof=true&sd=true" target="_blank">Slides</a>]
            <br>
            Collaborating with clients from Apple, developed an object-agnostic gripping strategy for E-waste manipulation at high speeds and accuracy. Co-led the simulation team to integrate a 6-DoF parallel manipulator with MuJoCo, using a waypoint-following PD controller, achieving joint value errors under 8 mm (lin.) and 0.1 radians (rot.). Created a numerical optimization-based forward kinematics approach with maximum errors of 5 mm (pos.) and 0.75 radians (orient.). Built a pseudo-Jacobian matrix for singularity avoidance and implemented a DDPG + HER-based Deep RL algorithm in MuJoCo-powered Robosuite, reaching a 95.55% success rate up to 0.5 m/s conveyor speeds.
          </p>
        </div>
      </td>
    </tr>
    <!-- ! ROB 550 MBot-->
    <tr>
      <td>
        <div class="project_cell">
          <img src="assets/img/mbot_slam_gif.gif" width="250" height="200" class="zoom">
        </div>
        <div class="project_cell">
          <p style="font-size:18px;margin-bottom: 0px;">
            <font color="black"><b>MBot Autonomy: Control, Perception, and Navigation</b></font>
          </p>
          <p>
            [<a href="https://github.com/SuryaPratapSingh37/Robot-Navigation-SLAM" target="_blank">Code</a>]
            [<a href="https://drive.google.com/file/d/1_I9DzmosTZh322cdZhQ5kC2oYU9Sklzp/view?usp=drive_link" target="_blank">PDF</a>]
            <br>
            To develop a comprehensive robotic system for autonomous navigation in a warehouse scenario, a multi-hierarchical motion controller was implemented, achieving 2.5 cm RMS trajectory error and velocity errors of 0.025 m/s (linear) and 0.045 rad/s (angular). SLAM was deployed with 2D occupancy grid mapping and Monte Carlo localization, resulting in a 6.63 cm RMS localization error relative to gyrodometry. An A* path planner with collision avoidance and frontier exploration ensured 100% success in optimal path-finding, returning within 5 cm of the home position.
          </p>
        </div>
      </td>
    </tr>
    
    <!-- ! ROB550 Armlab -->
    <tr>
      <td>
        <div class="project_cell">
          <img src="assets/img/armlab_block_sorting_gif.gif" width="250" height="200" class="zoom">
        </div>
        <div class="project_cell">
          <p style="font-size:18px;margin-bottom: 0px;">
            <font color="black"><b>Autonomous Robotic Arm: Vision-Guided Manipulation</b></font>
          </p>
          <p>
            [<a href="https://github.com/SuryaPratapSingh37/Robot-Navigation-SLAM" target="_blank">Code</a>]
            [<a href="https://drive.google.com/file/d/19UP8UG3t_4UYOeVGo8vzhoAtccAKI-aY/view?usp=sharing" target="_blank">PDF</a>]
            <br>
            To enable human users to control the RX200 arm through a GUI while providing autonomy via vision-guided kinematics, the arm and RGB-D camera were integrated into the ROS2 framework. A PID controller was implemented for precise control, achieving less than 2 cm end-effector positioning error in block swapping. Automatic camera calibration with Apriltags and integration with forward and inverse kinematics ensured 100% success in click-to-grab/drop tasks within a 35 cm radius. Block color (VIBGYOR) and size detection algorithms with OpenCV allowed the arm to sort blocks with 100% accuracy.
          </p>
        </div>
      </td>
    </tr>

    <!-- ! Self Driving Cars -->
    <tr>
      <td>
        <div class="project_cell">
          <img src="assets/img/self_driving_suite_gif.gif" width="250" height="200" class="zoom">
        </div>
        <div class="project_cell">
          <p style="font-size:18px;margin-bottom: 0px;">
            <font color="black"><b>SelfDriveSuite: Vehicle Control and Scene Understanding</b></font>
          </p>
          <p>
            [<a href="https://github.com/SuryaPratapSingh37/Robot-Navigation-SLAM" target="_blank">Code</a>]
            <br>
            This project developed algorithms for autonomous driving, including adaptive cruise control, model predictive control (MPC), and robust scene understanding. A QP-based minimum norm controller ensured a safe following distance of 1.8 times the vehicle's velocity. Linear and nonlinear MPCs achieved path tracking with RMS errors of &it1m in position, &it0.5 rad in orientation, &it0.5 m/s in velocity, and &it1 m/s² in acceleration. For scene understanding, a 2D image classification model reached 90% accuracy on blurry images, and a 15-class scene segmentation model, trained in varied weather conditions, achieved 81.2 mIoU using UNet.
          </p>
        </div>
      </td>
    </tr>

    <!-- ! OriCon3D -->
    <tr>
      <td>
        <div class="project_cell">
          <img src="assets/img/oricon3d_gif.gif" width="250" height="200" class="zoom">
        </div>
        <div class="project_cell">
          <p style="font-size:18px;margin-bottom: 0px;">
            <font color="black"><b>OriCon3D: Monocular 3D Object Detection</b></font>
          </p>
          <p>
            [<a href="https://github.com/DhyeyR-007/OriCon3D/tree/main" target="_blank">Code</a>]
            [<a href="https://arxiv.org/pdf/2304.14484" target="_blank">PDF</a>]
            <br>
            This project involved developing a robust and lightweight 3D object detection architecture for real-time autonomous driving using the KITTI dataset. Using MultiBin to regress 3D bounding box orientation from 2D bounding boxes with a pre-trained YOLO model, we achieved 3D IoU scores of 76.9 for Cars, 67.76 for Pedestrians, and 66.5 for Cyclists. Integrating lightweight feature extractors like MobileNet-v2 and EfficientNet-v2 reduced inference time by over 80% and improved 3D IoU by 2.4% for Cars, 1.5% for Pedestrians, and 4.5% for Cyclists, delivering superior performance compared to conventional methods.
          </p>
        </div>
      </td>
    </tr>

    <!-- ! Twilight-SLAM -->
    <tr>
      <td>
        <div class="project_cell">
          <img src="assets/img/twilight_slam_gif.gif" width="250" height="200" class="zoom">
        </div>
        <div class="project_cell">
          <p style="font-size:18px;margin-bottom: 0px;">
            <font color="black"><b>Twilight SLAM: Navigating Low-Light Environments</b></font>
          </p>
          <p>
            [<a href="https://github.com/TwilightSLAM" target="_blank">Code</a>]
            [<a href="https://arxiv.org/pdf/2304.11310" target="_blank">PDF</a>]
            <br>
            This project enhanced Visual SLAM accuracy by integrating image enhancement modules—Bread, Dual, EnlightenGAN, and Zero-DCE—into ORB-SLAM3 and SuperPoint-SLAM. This achieved a 15% mean increase in features extracted across various lighting conditions. Localization precision improved with an 18% reduction in RMSE between SLAM-generated and ground-truth poses, with ORB-SLAM3 outperforming SuperPoint-SLAM. EnlightenGAN with ORB-SLAM3 also reduced maximum absolute error in SLAM-generated poses by 13%, optimizing performance in low-light conditions.
          </p>
        </div>
      </td>
    </tr>

    <!-- ! Autonomous UAV -->
    <tr>
      <td>
        <div class="project_cell">
          <img src="assets/img/uav_gif.gif" width="250" height="200" class="zoom">
        </div>
        <div class="project_cell">
          <p style="font-size:18px;margin-bottom: 0px;">
            <font color="black"><b>Autonomous UAV-based Search and Rescue</b></font>
          </p>
          <p>
            [<a href="https://github.com/SuryaPratapSingh37/UAV-SaR-Tracking" target="_blank">Code</a>]
            [<a href="https://youtu.be/EpdRJuqJbFY?t=1551" target="_blank">Video</a>]
            [<a href="https://ieeexplore.ieee.org/abstract/document/9292630" target="_blank">PDF</a>]
            <br>
            To enhance autonomous UAV navigation in search-and-rescue (SaR) missions, the project focused on using reinforcement learning to locate and track victims until the rescue team arrives. An urban scenario was simulated in Gazebo with the AR.Drone, and a position controller was fine-tuned using MATLAB’s System Identification Toolbox, achieving less than 1 cm position error. Q-learning with function approximation reduced training time by 50% and addressed large state spaces. YOLO combined with Optical Flow was also used for real-time target tracking, showcasing effective performance in dynamic SaR environments.
          </p>
        </div>
      </td>
    </tr>
  </tbody>
</table>
