<table class="projects">
  <tbody>
    <tr>
      <td>
        <div class="project_cell">
          <img src="assets/img/convbki_grounded_sam2_gif.gif" width="250" height="200" class="zoom">
        </div>
        <div class="project_cell">
          <p style="font-size:18px;margin-bottom: 0px;">
            <font color="black"><b>Autonomous Surface Vehicle</b></font>
          </p>
          <p>
            <!-- [<span style="color: grey; text-decoration: none;">Code (Coming Soon)</span>] -->
            [<a href="https://github.com/spsingh37/BKI_ROS/tree/ros2_grounded_sam2" target="_blank">Code</a>]
            [<a href="https://drive.google.com/file/d/1QXohIqUfvDyveIPuflxW92zDKY5-wH4f/view?usp=sharing" target="_blank">Video</a>]
          </p>
          <p><b>Key Skills & Technologies:</b> 
            Autonomy, Machine Learning / Deep Learning, 3D Semantic Mapping, Convolutional Bayesian Kernel Inference, LiDAR Point Cloud Processing, Object Detection & Segmentation, Grounded SAM2, 3D-to-2D Projection, DROID-SLAM, Invariant Extended Kalman Filter, DRIFT, IMU-based Preintegration, GPS-based Correction, Ardupilot, MAVLink, PyTorch, ROS2, C++, Python 
          </p>
          <details>
            <summary style="cursor: pointer; color: #6bb3fb; text-decoration: underline;">Description</summary>
            <p>
              A full-stack autonomy system is being developed for a marine surface vehicle. ROS2-to-MAVLink communication enables seamless interaction between software and hardware, while an IMU, GPS, LiDAR, and Stereo RGB Camera are integrated with the ArduPilot framework. For localization, DROID-SLAM with SAM2 (to mask water) initially achieved ~1.5m error, later reduced to < 20 cm using the DRIFT algorithm with IMU preintegration and GPS correction. To construct a 3D semantic map, Convolutional Bayesian Kernel Inference was applied to LiDAR point clouds, with labels generated by projecting points onto Grounded SAM2-generated 2D segmentation masks.
            </p>
          </details>
        </div>
      </td>
    </tr>
    <tr>
      <td>
        <div class="project_cell">
          <img src="assets/img/multimodal_perception_cover_gif.gif" width="250" height="200" class="zoom">
        </div>
        <div class="project_cell">
          <p style="font-size:18px;margin-bottom: 0px;">
            <font color="black"><b>Multimodal Perception for Autonomous Racing</b></font>
          </p>
          <p>
            [<span style="color: grey; text-decoration: none;">Code (Coming Soon)</span>]
            [<a href="https://drive.google.com/file/d/1stXqYokZfWw-clVRVku7g8p80hlcKNYL/view?usp=sharing" target="_blank">Video</a>]
          </p>
          <p><b>Key Skills & Technologies:</b> 
            Multimodal Scene Understanding, Machine Learning / Deep Learning, Image Segmentation, UNet, Variational Autoencoder (VAE), Mixture of Softmaxes, Online & Offline RL, TD-MPC, DreamerV3, ROS, ROS Nodelet, Docker, PyTorch, Python
          </p>
          <details>
            <summary style="cursor: pointer; color: #6bb3fb; text-decoration: underline;">Description</summary>
            <p>
              A robust multi-terrain control policy for autonomous racing was developed using LiDAR and RGB camera inputs via reinforcement learning. To enhance visual sim-to-real transfer, a lightweight UNet model segmented key features—floor, opponent rover, walls, and background—achieving a 0.99 Dice score. ROS nodelet optimization and ROS-Docker communication improvements increased update rates from 10 Hz to 30 Hz, enabling the rover to reach speeds of 2.5 m/s. Self-supervised sensor fusion using a Mixture of Softmaxes (MoS) effectively combined LiDAR and RGB data, leading to 60% head-to-head race wins against a dynamic rule-based gap-follower agent. Finally, offline TD-MPC reinforcement learning delivered strong policies in under 15 episodes with only 5K-10K transitions, significantly reducing sample complexity compared to over 300K episodes required for online training.
            </p>
          </details>
        </div>
      </td>
    </tr>
    <tr>
      <td>
        <div class="project_cell">
          <img src="assets/img/wombat_results_gif.gif" width="250" height="200" class="zoom">
        </div>
        <div class="project_cell">
          <p style="font-size:18px;margin-bottom: 0px;">
            <font color="black"><b>Electronic-waste Recycling</b></font>
          </p>
          <p>
            [<a href="https://github.com/biorobotics/Wombat_robosuite" target="_blank">Code</a>]
            [<a href="https://docs.google.com/presentation/d/1uCb-5oLUr9LShpgxhDzhjrvFSEvU9o_i/edit?usp=sharing&ouid=105419178815487827350&rtpof=true&sd=true" target="_blank">Slides</a>]
          </p>
          <p><b>Key Skills & Technologies:</b> 
            6-DoF Parallel Manipulator, MuJoCo, Robosuite, Numerical Optimization, Forward Kinematics, PD Controller, Singularity Avoidance, Multi-Object Tracking, Classical Computer Vision, OpenCV, SORT Tracker, Machine Learning / Deep Learning, DDPG, Hindsight Experience Replay (HER), PyTorch, Python
          </p>
          <details>
            <summary style="cursor: pointer; color: #6bb3fb; text-decoration: underline;">Description</summary>
            <p>
              Collaborating with clients from Apple, developed an object-agnostic gripping strategy for E-waste manipulation at high speeds and accuracy. Co-led the simulation team to integrate a 6-DoF parallel manipulator with MuJoCo, using a waypoint-following PD controller, achieving joint value errors under 8 mm (lin.) and 0.1 radians (rot.). Created a numerical optimization-based forward kinematics approach with maximum errors of 5 mm (pos.) and 0.75 radians (orient.). Built a pseudo-Jacobian matrix for singularity avoidance. Developed a multi-object tracking module using classical computer vision techniques with OpenCV and the SORT tracker. Implemented a DDPG + HER-based Deep RL algorithm in MuJoCo-powered Robosuite, reaching a 95.55% success rate at conveyor speeds up to 0.5 m/s.
            </p>
          </details>
        </div>
      </td>
    </tr>
    <!-- ! ROB 550 MBot-->
    <tr>
      <td>
        <div class="project_cell">
          <img src="assets/img/mbot_slam_gif.gif" width="250" height="200" class="zoom">
        </div>
        <div class="project_cell">
          <p style="font-size:18px;margin-bottom: 0px;">
            <font color="black"><b>MBot Autonomy: Control, Perception, and Navigation</b></font>
          </p>
          <p>
            [<a href="https://github.com/SuryaPratapSingh37/Robot-Navigation-SLAM" target="_blank">Code</a>]
            [<a href="https://drive.google.com/file/d/1_I9DzmosTZh322cdZhQ5kC2oYU9Sklzp/view?usp=drive_link" target="_blank">PDF</a>]
          </p>
          <p><b>Key Skills & Technologies:</b> 
            Autonomous Navigation, Lightweight Communications and Marshalling (LCM), Multi-Hierarchical Motion Controller, SLAM, Monte Carlo Localization, Dead reckoning, Gyroscope + Wheel encoder-based Odometry, 2D Occupancy Grid Mapping, A* Path Planning, Frontier Exploration Offline Reinforcement Learning, Sensor Fusion, Docker, UNet Model Development, Python, C
          </p>
          <details>
            <summary style="cursor: pointer; color: #6bb3fb; text-decoration: underline;">Description</summary>
            <p>
              A full-stack robotic system was developed for autonomous warehouse navigation using an MBot platform. A multi-hierarchical motion controller achieved a 2.5 cm RMS trajectory error with velocity errors of 0.025 m/s (linear) and 0.045 rad/s (angular). SLAM, with 2D occupancy grid mapping and Monte Carlo localization, yielded a 6.63 cm RMS localization error relative to gyrodometry. An A* path planner with collision avoidance and frontier exploration enabled 100% success in optimal path-finding, consistently returning within 5 cm of the home position.
            </p>
          </details>
        </div>
      </td>
    </tr>
    
    <!-- ! ROB550 Armlab -->
    <tr>
      <td>
        <div class="project_cell">
          <img src="assets/img/armlab_block_sorting_gif.gif" width="250" height="200" class="zoom">
        </div>
        <div class="project_cell">
          <p style="font-size:18px;margin-bottom: 0px;">
            <font color="black"><b>Autonomous Robotic Arm: Vision-Guided Manipulation</b></font>
          </p>
          <p>
            [<a href="https://github.com/SuryaPratapSingh37/Robot-Navigation-SLAM" target="_blank">Code</a>]
            [<a href="https://drive.google.com/file/d/19UP8UG3t_4UYOeVGo8vzhoAtccAKI-aY/view?usp=sharing" target="_blank">PDF</a>]
          </p>
          <p><b>Key Skills & Technologies:</b> 
            Manipulation, Computer Vision, GUI Development, PID Controller, Forward & Inverse Kinematics, RGB-D Camera, Camera Calibration, AprilTags, Object Detection & Sorting, OpenCV, ROS2, Python
          </p>
          <details>
            <summary style="cursor: pointer; color: #6bb3fb; text-decoration: underline;">Description</summary>
            <p>
              A GUI-based control and vision-guided autonomy system was developed for the RX200 arm. The arm and an RGB-D camera were integrated into the ROS2 framework, with a PID controller achieving sub-2 cm end-effector positioning error in block swapping. Automatic camera calibration using AprilTags, along with forward and inverse kinematics, enabled 100% success in click-to-grab/drop tasks within a 35 cm radius. OpenCV-based algorithms for block color (VIBGYOR) and size detection ensured 100% accuracy in sorting.
            </p>
          </details>
        </div>
      </td>
    </tr>

    <!-- ! Self Driving Cars -->
    <tr>
      <td>
        <div class="project_cell">
          <img src="assets/img/self_driving_suite_gif.gif" width="250" height="200" class="zoom">
        </div>
        <div class="project_cell">
          <p style="font-size:18px;margin-bottom: 0px;">
            <font color="black"><b>SelfDriveSuite: Vehicle Control and Scene Understanding</b></font>
          </p>
          <p>
            [<a href="https://github.com/SuryaPratapSingh37/Robot-Navigation-SLAM" target="_blank">Code</a>]
          </p>
          <p><b>Key Skills & Technologies:</b> 
            Adaptive Cruise Control, Model Predictive Control, Optimization (Quadratic Programming), Image Classification, Semantic Segmentation, UNet, Machine Learning / Deep Learning, PyTorch, Python
          </p>
          <details>
            <summary style="cursor: pointer; color: #6bb3fb; text-decoration: underline;">Description</summary>
            <p>
              This project developed autonomous driving algorithms, integrating adaptive cruise control, model predictive control (MPC), and robust scene understanding. A QP-based minimum-norm controller maintained a safe following distance of 1.8x velocity. Linear and nonlinear MPCs achieved precise path tracking with RMS errors of < 1 m (position), < 0.5 rad (orientation), < 0.5 m/s (velocity), and < 1 m/s² (acceleration). For scene understanding, a 2D image classifier reached 90% accuracy on blurry images, while a 15-class UNet-based segmentation model, trained in varied weather conditions, achieved 81.2% mIoU.
            </p>
          </details>
        </div>
      </td>
    </tr>

    <!-- ! OriCon3D -->
    <tr>
      <td>
        <div class="project_cell">
          <img src="assets/img/oricon3d_gif.gif" width="250" height="200" class="zoom">
        </div>
        <div class="project_cell">
          <p style="font-size:18px;margin-bottom: 0px;">
            <font color="black"><b>OriCon3D: Monocular 3D Object Detection</b></font>
          </p>
          <p>
            [<a href="https://github.com/DhyeyR-007/OriCon3D/tree/main" target="_blank">Code</a>]
            [<a href="https://arxiv.org/pdf/2304.14484" target="_blank">PDF</a>]
          </p>
          <p><b>Key Skills & Technologies:</b> 
            3D Object Detection, Machine Learning / Deep Learning, YOLO, MobileNet-v2, EfficientNet-v2, KITTI Dataset, PyTorch, Python
          </p>
          <details>
            <summary style="cursor: pointer; color: #6bb3fb; text-decoration: underline;">Description</summary>
            <p>
              This project developed a lightweight 3D object detection architecture for real-time autonomous driving using the KITTI dataset. A pre-trained YOLO model with MultiBin regression for 3D bounding box orientation achieved 3D IoU scores of 76.9 (Cars), 67.76 (Pedestrians), and 66.5 (Cyclists). Integrating efficient feature extractors like MobileNet-v2 and EfficientNet-v2 reduced inference time by over 80% and improved 3D IoU by 2.4% (Cars), 1.5% (Pedestrians), and 4.5% (Cyclists), outperforming conventional methods.
            </p>
          </details>
        </div>
      </td>
    </tr>

    <!-- ! Twilight-SLAM -->
    <tr>
      <td>
        <div class="project_cell">
          <img src="assets/img/twilight_slam_gif.gif" width="250" height="200" class="zoom">
        </div>
        <div class="project_cell">
          <p style="font-size:18px;margin-bottom: 0px;">
            <font color="black"><b>Twilight SLAM: Navigating Low-Light Environments</b></font>
          </p>
          <p>
            [<a href="https://github.com/TwilightSLAM" target="_blank">Code</a>]
            [<a href="https://arxiv.org/pdf/2304.11310" target="_blank">PDF</a>]
          </p>
          <p><b>Key Skills & Technologies:</b> 
            Low-Light SLAM, ORB-SLAM3, SuperPoint-SLAM, Feature Extraction, Image Enhancement, Model Optimization, ONNX, ONNXRuntime, C++, Python
          </p>
          <details>
            <summary style="cursor: pointer; color: #6bb3fb; text-decoration: underline;">Description</summary>
            <p>
              Enhanced Visual SLAM accuracy by integrating image enhancement models (Bread, Dual, EnlightenGAN, Zero-DCE) into ORB-SLAM3 and SuperPoint-SLAM. This increased feature extraction by 15% across varied lighting conditions and improved localization with an 18% RMSE reduction between SLAM-generated and ground-truth poses. ORB-SLAM3 outperformed SuperPoint-SLAM, with EnlightenGAN reducing maximum absolute pose error by 13% in low-light scenarios. For real-time performance, EnlightenGAN's ONNX model was built, achieving a 3x speed boost using ONNXRuntime.
            </p>
          </details>
        </div>
      </td>
    </tr>

    <!-- ! Autonomous UAV -->
    <tr>
      <td>
        <div class="project_cell">
          <img src="assets/img/uav_gif.gif" width="250" height="200" class="zoom">
        </div>
        <div class="project_cell">
          <p style="font-size:18px;margin-bottom: 0px;">
            <font color="black"><b>Autonomous UAV-based Search and Rescue</b></font>
          </p>
          <p>
            [<a href="https://github.com/SuryaPratapSingh37/UAV-SaR-Tracking" target="_blank">Code</a>]
            [<a href="https://youtu.be/EpdRJuqJbFY?t=1551" target="_blank">Video</a>]
            [<a href="https://ieeexplore.ieee.org/abstract/document/9292630" target="_blank">PDF</a>]
          </p>
          <p><b>Key Skills & Technologies:</b> 
            Q-learning, Function Approximation, MATLAB, System Identification Toolbox, PID tuning, Gazebo, ROS, Python
          </p>
          <details>
            <summary style="cursor: pointer; color: #6bb3fb; text-decoration: underline;">Description</summary>
            <p>
            To enhance autonomous UAV navigation in search-and-rescue (SaR) missions, the project focused on using reinforcement learning to locate and track victims until the rescue team arrives. An urban scenario was simulated in Gazebo with the AR.Drone, and a position controller was fine-tuned using MATLAB’s System Identification Toolbox, achieving less than 1 cm position error. Q-learning with function approximation reduced training time by 50% and addressed large state spaces. YOLO combined with Optical Flow was also used for real-time target tracking, showcasing effective performance in dynamic SaR environments.
            </p>
          </details>
        </div>
      </td>
    </tr>
  </tbody>
</table>
